{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee2d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running spider for: Aberystwyth University.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gspread_dataframe import get_as_dataframe\n",
    "import gspread\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# Directories\n",
    "# ---------------------------\n",
    "INPUT_DIR = \"Downloaded_Universities\"\n",
    "PROCESSED_DIR = os.path.join(INPUT_DIR, \"Processed_Universities\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Google Sheet Config\n",
    "# ---------------------------\n",
    "SERVICE_ACCOUNT_FILE = \"service_account.json\"\n",
    "SHEET_ID = \"1eYz8Nvr3BToRrmReXNLR8zQrk4X8tsdKZO_Fj9mNThc\"\n",
    "SHEET_NAME = \"Scrapper Running\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    \"\"\"Clean a string to make it safe for file names.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    return re.sub(r'[.\\'<>:\"/\\\\|?*]', '', name).strip().lower()\n",
    "\n",
    "\n",
    "def update_sheet_status(csv_file_name, status):\n",
    "    \"\"\"Update 'Scraping?' column for matching university CSV in the sheet.\"\"\"\n",
    "    uni_name_from_csv = safe_filename(os.path.splitext(csv_file_name)[0])\n",
    "\n",
    "    for i, uni_name in df_sheet['University Name'].items():\n",
    "        if uni_name and safe_filename(str(uni_name)) == uni_name_from_csv:\n",
    "            df_sheet.at[i, 'Scraping?'] = status\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Authenticate Google Sheet\n",
    "# ---------------------------\n",
    "gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)\n",
    "sh = gc.open_by_key(SHEET_ID)\n",
    "ws = sh.worksheet(SHEET_NAME)\n",
    "df_sheet = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
    "\n",
    "# Ensure 'Scraping?' column exists\n",
    "if 'Scraping?' not in df_sheet.columns:\n",
    "    df_sheet['Scraping?'] = \"Pending\"\n",
    "\n",
    "# ---------------------------\n",
    "# Spider runner\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def run_spider_on_csvs():\n",
    "    csv_files = [f for f in os.listdir(\n",
    "        INPUT_DIR) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(INPUT_DIR, csv_file)\n",
    "        print(f\"üöÄ Running spider for: {csv_file}\")\n",
    "\n",
    "        # Mark as processing\n",
    "        update_sheet_status(csv_file, \"Processing\")\n",
    "        ws.update([df_sheet.columns.values.tolist()] +\n",
    "                  df_sheet.fillna(\"\").values.tolist())\n",
    "\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"python\", \"-m\", \"scrapy\", \"crawl\", \"courses\", f\"-a\", f\"csv_file={csv_path}\"\n",
    "            ], check=True)\n",
    "\n",
    "            # Move processed CSV\n",
    "            shutil.move(csv_path, os.path.join(PROCESSED_DIR, csv_file))\n",
    "            print(f\"‚úÖ Finished and moved: {csv_file}\")\n",
    "\n",
    "            # Mark as processed\n",
    "            update_sheet_status(csv_file, \"Processed\")\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Spider failed for {csv_file}: {e}\")\n",
    "            update_sheet_status(csv_file, \"Failed\")\n",
    "\n",
    "    # Push final status to Google Sheet\n",
    "    ws.update([df_sheet.columns.values.tolist()] +\n",
    "              df_sheet.fillna(\"\").values.tolist())\n",
    "    print(\"‚úÖ Sheet updated with Scraping? statuses\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run all CSVs\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_spider_on_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdeb4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
