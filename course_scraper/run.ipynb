{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee2d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf90f99ddd60494581dd91f02711222e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSVs:   0%|          | 0/323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running spider for: Boston College.csv\n",
      "‚úÖ Finished and moved: Boston College.csv\n",
      "‚è±Ô∏è Time elapsed for Boston College.csv: 18.89 seconds\n",
      "\n",
      "üöÄ Running spider for: Bournemouth University.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from gspread_dataframe import get_as_dataframe\n",
    "import gspread\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "\n",
    "# ---------------------------\n",
    "# Directories\n",
    "# ---------------------------\n",
    "INPUT_DIR = \"Downloaded_Universities\"\n",
    "PROCESSED_DIR = os.path.join(INPUT_DIR, \"Processed_Universities\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Google Sheet Config\n",
    "# ---------------------------\n",
    "SERVICE_ACCOUNT_FILE = \"service_account.json\"\n",
    "SHEET_ID = \"1eYz8Nvr3BToRrmReXNLR8zQrk4X8tsdKZO_Fj9mNThc\"\n",
    "SHEET_NAME = \"Scrapper Running\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def log_time(csv_file, elapsed_seconds):\n",
    "    \"\"\"Append elapsed time info for a university to a daily log file in H:M:S format.\"\"\"\n",
    "    uni_name = os.path.splitext(csv_file)[0]\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    hours, remainder = divmod(elapsed_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    elapsed_str = f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\"\n",
    "\n",
    "    log_file = os.path.join(\n",
    "        LOG_DIR, f\"elapsed_{datetime.now(timezone.utc).strftime('%Y-%m-%d')}.log\")\n",
    "\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{ts}] {uni_name}: {elapsed_str}\\n\")\n",
    "\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    \"\"\"Clean a string to make it safe for file names.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    return re.sub(r'[.\\'<>:\"/\\\\|?*]', '', name).strip().lower()\n",
    "\n",
    "\n",
    "def update_sheet_status(csv_file_name, status):\n",
    "    \"\"\"Update 'Scraping?' column for matching university CSV in the sheet.\"\"\"\n",
    "    uni_name_from_csv = safe_filename(os.path.splitext(csv_file_name)[0])\n",
    "\n",
    "    for i, uni_name in df_sheet['University Name'].items():\n",
    "        if uni_name and safe_filename(str(uni_name)) == uni_name_from_csv:\n",
    "            df_sheet.at[i, 'Scraping?'] = status\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Authenticate Google Sheet\n",
    "# ---------------------------\n",
    "gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)\n",
    "sh = gc.open_by_key(SHEET_ID)\n",
    "ws = sh.worksheet(SHEET_NAME)\n",
    "df_sheet = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
    "\n",
    "# Ensure 'Scraping?' column exists\n",
    "if 'Scraping?' not in df_sheet.columns:\n",
    "    df_sheet['Scraping?'] = \"Pending\"\n",
    "\n",
    "# ---------------------------\n",
    "# Spider runner\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def run_spider_on_csvs():\n",
    "    csv_files = [f for f in os.listdir(\n",
    "        INPUT_DIR) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "    # Create a single notebook tqdm bar\n",
    "    pbar = tqdm(total=len(csv_files), desc=\"Processing CSVs\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(INPUT_DIR, csv_file)\n",
    "        tqdm.write(f\"\\nüöÄ Running spider for: {csv_file}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Mark as processing\n",
    "        update_sheet_status(csv_file, \"Processing\")\n",
    "        ws.update([df_sheet.columns.values.tolist()] +\n",
    "                  df_sheet.fillna(\"\").values.tolist())\n",
    "\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"python\", \"-m\", \"scrapy\", \"crawl\", \"courses\",\n",
    "                f\"-a\", f\"csv_file={csv_path}\"\n",
    "            ], check=True)\n",
    "\n",
    "            # Move processed CSV\n",
    "            shutil.move(csv_path, os.path.join(PROCESSED_DIR, csv_file))\n",
    "            print(f\"‚úÖ Finished and moved: {csv_file}\")\n",
    "\n",
    "            # Count number of .txt files in the output folder (same as CSV name)\n",
    "            output_folder = os.path.splitext(csv_file)[0]\n",
    "            txt_files_count = 0\n",
    "            folder_path = os.path.join(output_folder)\n",
    "            if os.path.exists(folder_path):\n",
    "                txt_files_count = len([f for f in os.listdir(\n",
    "                    folder_path) if f.lower().endswith(\".txt\")])\n",
    "\n",
    "            # Update Google Sheet 'Scraped Count' column\n",
    "            uni_name_from_csv = safe_filename(os.path.splitext(csv_file)[0])\n",
    "            for i, uni_name in df_sheet['University Name'].items():\n",
    "                if uni_name and safe_filename(str(uni_name)) == uni_name_from_csv:\n",
    "                    df_sheet.at[i, 'Scraped Count'] = txt_files_count\n",
    "                    df_sheet.at[i, 'Scraping?'] = \"Processed\"\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Spider failed for {csv_file}: {e}\")\n",
    "            update_sheet_status(csv_file, \"Failed\")\n",
    "            txt_files_count = 0\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        tqdm.write(f\"‚è±Ô∏è Time elapsed for {csv_file}: {elapsed:.2f} seconds\")\n",
    "\n",
    "        # log to file\n",
    "        log_time(csv_file, elapsed)\n",
    "\n",
    "        # Update the same tqdm bar\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Push final status to Google Sheet including Scraped Count\n",
    "    ws.update([df_sheet.columns.values.tolist()] +\n",
    "              df_sheet.fillna(\"\").values.tolist())\n",
    "    print(\"‚úÖ Sheet updated with Scraping? and Scraped Count statuses\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run all CSVs\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_spider_on_csvs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
