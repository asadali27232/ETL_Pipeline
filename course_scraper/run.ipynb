{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee2d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ffb03246e6454680d3f741d5524736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSVs:   0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running spider for: Bow Valley College.csv\n",
      "✅ Finished and moved: Bow Valley College.csv\n",
      "⏱️ Time elapsed for Bow Valley College.csv: 94.25 seconds\n",
      "\n",
      "🚀 Running spider for: Bradley University.csv\n",
      "✅ Finished and moved: Bradley University.csv\n",
      "⏱️ Time elapsed for Bradley University.csv: 232.68 seconds\n",
      "\n",
      "🚀 Running spider for: Brandon University.csv\n",
      "✅ Finished and moved: Brandon University.csv\n",
      "⏱️ Time elapsed for Brandon University.csv: 20.21 seconds\n",
      "\n",
      "🚀 Running spider for: British Columbia Institute of technology.csv\n",
      "✅ Finished and moved: British Columbia Institute of technology.csv\n",
      "⏱️ Time elapsed for British Columbia Institute of technology.csv: 67.80 seconds\n",
      "\n",
      "🚀 Running spider for: Brock University.csv\n",
      "✅ Finished and moved: Brock University.csv\n",
      "⏱️ Time elapsed for Brock University.csv: 126.04 seconds\n",
      "\n",
      "🚀 Running spider for: Brunel University London.csv\n",
      "✅ Finished and moved: Brunel University London.csv\n",
      "⏱️ Time elapsed for Brunel University London.csv: 64.85 seconds\n",
      "\n",
      "🚀 Running spider for: Ca foscari university of venice.csv\n",
      "✅ Finished and moved: Ca foscari university of venice.csv\n",
      "⏱️ Time elapsed for Ca foscari university of venice.csv: 49.36 seconds\n",
      "\n",
      "🚀 Running spider for: California State University.csv\n",
      "✅ Finished and moved: California State University.csv\n",
      "⏱️ Time elapsed for California State University.csv: 45.70 seconds\n",
      "\n",
      "🚀 Running spider for: Canadian Mennonite University.csv\n",
      "✅ Finished and moved: Canadian Mennonite University.csv\n",
      "⏱️ Time elapsed for Canadian Mennonite University.csv: 11.80 seconds\n",
      "\n",
      "🚀 Running spider for: Canadore College.csv\n",
      "✅ Finished and moved: Canadore College.csv\n",
      "⏱️ Time elapsed for Canadore College.csv: 93.37 seconds\n",
      "\n",
      "🚀 Running spider for: Capilano University.csv\n",
      "✅ Finished and moved: Capilano University.csv\n",
      "⏱️ Time elapsed for Capilano University.csv: 25.16 seconds\n",
      "\n",
      "🚀 Running spider for: Cardiff Metropoliton University.csv\n",
      "✅ Finished and moved: Cardiff Metropoliton University.csv\n",
      "⏱️ Time elapsed for Cardiff Metropoliton University.csv: 86.40 seconds\n",
      "\n",
      "🚀 Running spider for: Cardiff University.csv\n",
      "✅ Finished and moved: Cardiff University.csv\n",
      "⏱️ Time elapsed for Cardiff University.csv: 799.82 seconds\n",
      "\n",
      "🚀 Running spider for: Charles Sturt University.csv\n",
      "✅ Finished and moved: Charles Sturt University.csv\n",
      "⏱️ Time elapsed for Charles Sturt University.csv: 116.40 seconds\n",
      "\n",
      "🚀 Running spider for: City University of London.csv\n",
      "✅ Finished and moved: City University of London.csv\n",
      "⏱️ Time elapsed for City University of London.csv: 1366.00 seconds\n",
      "\n",
      "🚀 Running spider for: Clark University.csv\n",
      "✅ Finished and moved: Clark University.csv\n",
      "⏱️ Time elapsed for Clark University.csv: 57.26 seconds\n",
      "\n",
      "🚀 Running spider for: Cleveland State University.csv\n",
      "✅ Finished and moved: Cleveland State University.csv\n",
      "⏱️ Time elapsed for Cleveland State University.csv: 34.45 seconds\n",
      "\n",
      "🚀 Running spider for: Concordia University of Edmonton.csv\n",
      "✅ Finished and moved: Concordia University of Edmonton.csv\n",
      "⏱️ Time elapsed for Concordia University of Edmonton.csv: 10.98 seconds\n",
      "\n",
      "🚀 Running spider for: Conestoga College.csv\n",
      "✅ Finished and moved: Conestoga College.csv\n",
      "⏱️ Time elapsed for Conestoga College.csv: 151.08 seconds\n",
      "\n",
      "🚀 Running spider for: Cornell University.csv\n",
      "✅ Finished and moved: Cornell University.csv\n",
      "⏱️ Time elapsed for Cornell University.csv: 137.46 seconds\n",
      "\n",
      "🚀 Running spider for: Coventry University.csv\n",
      "✅ Finished and moved: Coventry University.csv\n",
      "⏱️ Time elapsed for Coventry University.csv: 75.22 seconds\n",
      "\n",
      "🚀 Running spider for: CQ University.csv\n",
      "✅ Finished and moved: CQ University.csv\n",
      "⏱️ Time elapsed for CQ University.csv: 121.01 seconds\n",
      "\n",
      "🚀 Running spider for: Craleton University.csv\n",
      "✅ Finished and moved: Craleton University.csv\n",
      "⏱️ Time elapsed for Craleton University.csv: 57.21 seconds\n",
      "\n",
      "🚀 Running spider for: Crandall University.csv\n",
      "✅ Finished and moved: Crandall University.csv\n",
      "⏱️ Time elapsed for Crandall University.csv: 8.22 seconds\n",
      "\n",
      "🚀 Running spider for: Cranfield University.csv\n",
      "✅ Finished and moved: Cranfield University.csv\n",
      "⏱️ Time elapsed for Cranfield University.csv: 89.32 seconds\n",
      "\n",
      "🚀 Running spider for: Dalhousie University.csv\n",
      "✅ Finished and moved: Dalhousie University.csv\n",
      "⏱️ Time elapsed for Dalhousie University.csv: 17.95 seconds\n",
      "\n",
      "🚀 Running spider for: De Montfort University Leicester.csv\n",
      "✅ Finished and moved: De Montfort University Leicester.csv\n",
      "⏱️ Time elapsed for De Montfort University Leicester.csv: 29.72 seconds\n",
      "\n",
      "🚀 Running spider for: Deakin University.csv\n",
      "✅ Finished and moved: Deakin University.csv\n",
      "⏱️ Time elapsed for Deakin University.csv: 301.73 seconds\n",
      "\n",
      "🚀 Running spider for: DePaul University.csv\n",
      "✅ Finished and moved: DePaul University.csv\n",
      "⏱️ Time elapsed for DePaul University.csv: 89.73 seconds\n",
      "\n",
      "🚀 Running spider for: Drew University.csv\n",
      "✅ Finished and moved: Drew University.csv\n",
      "⏱️ Time elapsed for Drew University.csv: 92.63 seconds\n",
      "\n",
      "🚀 Running spider for: Duquesne university.csv\n",
      "✅ Finished and moved: Duquesne university.csv\n",
      "⏱️ Time elapsed for Duquesne university.csv: 60.74 seconds\n",
      "\n",
      "🚀 Running spider for: Durham College.csv\n",
      "✅ Finished and moved: Durham College.csv\n",
      "⏱️ Time elapsed for Durham College.csv: 47.26 seconds\n",
      "\n",
      "🚀 Running spider for: Eastern Institute of Technology.csv\n",
      "✅ Finished and moved: Eastern Institute of Technology.csv\n",
      "⏱️ Time elapsed for Eastern Institute of Technology.csv: 41.69 seconds\n",
      "\n",
      "🚀 Running spider for: Edge Hill University.csv\n",
      "✅ Finished and moved: Edge Hill University.csv\n",
      "⏱️ Time elapsed for Edge Hill University.csv: 1209.54 seconds\n",
      "\n",
      "🚀 Running spider for: Edinburgh Napier University.csv\n",
      "✅ Finished and moved: Edinburgh Napier University.csv\n",
      "⏱️ Time elapsed for Edinburgh Napier University.csv: 41.51 seconds\n",
      "\n",
      "🚀 Running spider for: Edith Cowan University.csv\n",
      "✅ Finished and moved: Edith Cowan University.csv\n",
      "⏱️ Time elapsed for Edith Cowan University.csv: 80.73 seconds\n",
      "\n",
      "🚀 Running spider for: Emily Carr University of Art and Design.csv\n",
      "✅ Finished and moved: Emily Carr University of Art and Design.csv\n",
      "⏱️ Time elapsed for Emily Carr University of Art and Design.csv: 8.50 seconds\n",
      "\n",
      "🚀 Running spider for: EU Business School.csv\n",
      "✅ Finished and moved: EU Business School.csv\n",
      "⏱️ Time elapsed for EU Business School.csv: 19.43 seconds\n",
      "\n",
      "🚀 Running spider for: Falmouth University.csv\n",
      "✅ Finished and moved: Falmouth University.csv\n",
      "⏱️ Time elapsed for Falmouth University.csv: 31.61 seconds\n",
      "\n",
      "🚀 Running spider for: Farleigh Dickinson University.csv\n",
      "✅ Finished and moved: Farleigh Dickinson University.csv\n",
      "⏱️ Time elapsed for Farleigh Dickinson University.csv: 413.55 seconds\n",
      "\n",
      "🚀 Running spider for: Federation University of Australia.csv\n",
      "✅ Finished and moved: Federation University of Australia.csv\n",
      "⏱️ Time elapsed for Federation University of Australia.csv: 27.38 seconds\n",
      "\n",
      "🚀 Running spider for: Fleming College.csv\n",
      "✅ Finished and moved: Fleming College.csv\n",
      "⏱️ Time elapsed for Fleming College.csv: 101.97 seconds\n",
      "\n",
      "🚀 Running spider for: Flinders University.csv\n",
      "✅ Finished and moved: Flinders University.csv\n",
      "⏱️ Time elapsed for Flinders University.csv: 422.06 seconds\n",
      "\n",
      "🚀 Running spider for: Florida Institute of Technology.csv\n",
      "✅ Finished and moved: Florida Institute of Technology.csv\n",
      "⏱️ Time elapsed for Florida Institute of Technology.csv: 23.80 seconds\n",
      "\n",
      "🚀 Running spider for: Florida International University.csv\n",
      "✅ Finished and moved: Florida International University.csv\n",
      "⏱️ Time elapsed for Florida International University.csv: 45.17 seconds\n",
      "\n",
      "🚀 Running spider for: Fordham University.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from gspread_dataframe import get_as_dataframe\n",
    "import gspread\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "\n",
    "# ---------------------------\n",
    "# Directories\n",
    "# ---------------------------\n",
    "INPUT_DIR = \"Downloaded_Universities\"\n",
    "PROCESSED_DIR = os.path.join(INPUT_DIR, \"Processed_Universities\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Google Sheet Config\n",
    "# ---------------------------\n",
    "SERVICE_ACCOUNT_FILE = \"service_account.json\"\n",
    "SHEET_ID = \"1eYz8Nvr3BToRrmReXNLR8zQrk4X8tsdKZO_Fj9mNThc\"\n",
    "SHEET_NAME = \"Scrapper Running\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def log_time(csv_file, elapsed_seconds):\n",
    "    \"\"\"Append elapsed time info for a university to a daily log file in H:M:S format.\"\"\"\n",
    "    uni_name = os.path.splitext(csv_file)[0]\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    hours, remainder = divmod(elapsed_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    elapsed_str = f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\"\n",
    "\n",
    "    log_file = os.path.join(\n",
    "        LOG_DIR, f\"elapsed_{datetime.now(timezone.utc).strftime('%Y-%m-%d')}.log\")\n",
    "\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{ts}] {uni_name}: {elapsed_str}\\n\")\n",
    "\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    \"\"\"Clean a string to make it safe for file names.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    return re.sub(r'[.\\'<>:\"/\\\\|?*]', '', name).strip().lower()\n",
    "\n",
    "\n",
    "def update_sheet_status(csv_file_name, status):\n",
    "    \"\"\"Update 'Scraping?' column for matching university CSV in the sheet.\"\"\"\n",
    "    uni_name_from_csv = safe_filename(os.path.splitext(csv_file_name)[0])\n",
    "\n",
    "    for i, uni_name in df_sheet['University Name'].items():\n",
    "        if uni_name and safe_filename(str(uni_name)) == uni_name_from_csv:\n",
    "            df_sheet.at[i, 'Scraping?'] = status\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Authenticate Google Sheet\n",
    "# ---------------------------\n",
    "gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)\n",
    "sh = gc.open_by_key(SHEET_ID)\n",
    "ws = sh.worksheet(SHEET_NAME)\n",
    "df_sheet = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
    "\n",
    "# Ensure 'Scraping?' column exists\n",
    "if 'Scraping?' not in df_sheet.columns:\n",
    "    df_sheet['Scraping?'] = \"Pending\"\n",
    "\n",
    "# ---------------------------\n",
    "# Spider runner\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def run_spider_on_csvs():\n",
    "    csv_files = [f for f in os.listdir(\n",
    "        INPUT_DIR) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "    # Create a single notebook tqdm bar\n",
    "    pbar = tqdm(total=len(csv_files), desc=\"Processing CSVs\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(INPUT_DIR, csv_file)\n",
    "        tqdm.write(f\"\\n🚀 Running spider for: {csv_file}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Mark as processing\n",
    "        update_sheet_status(csv_file, \"Processing\")\n",
    "        ws.update([df_sheet.columns.values.tolist()] +\n",
    "                  df_sheet.fillna(\"\").values.tolist())\n",
    "\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"python\", \"-m\", \"scrapy\", \"crawl\", \"courses\",\n",
    "                f\"-a\", f\"csv_file={csv_path}\"\n",
    "            ], check=True)\n",
    "\n",
    "            # Move processed CSV\n",
    "            shutil.move(csv_path, os.path.join(PROCESSED_DIR, csv_file))\n",
    "            print(f\"✅ Finished and moved: {csv_file}\")\n",
    "\n",
    "            # Count number of .txt files in the output folder (same as CSV name)\n",
    "            output_folder = os.path.splitext(csv_file)[0]\n",
    "            txt_files_count = 0\n",
    "            folder_path = os.path.join(output_folder)\n",
    "            if os.path.exists(folder_path):\n",
    "                txt_files_count = len([f for f in os.listdir(\n",
    "                    folder_path) if f.lower().endswith(\".txt\")])\n",
    "\n",
    "            # Update Google Sheet 'Scraped Count' column\n",
    "            uni_name_from_csv = safe_filename(os.path.splitext(csv_file)[0])\n",
    "            for i, uni_name in df_sheet['University Name'].items():\n",
    "                if uni_name and safe_filename(str(uni_name)) == uni_name_from_csv:\n",
    "                    df_sheet.at[i, 'Scraped Count'] = txt_files_count\n",
    "                    df_sheet.at[i, 'Scraping?'] = \"Processed\"\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Spider failed for {csv_file}: {e}\")\n",
    "            update_sheet_status(csv_file, \"Failed\")\n",
    "            txt_files_count = 0\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        tqdm.write(f\"⏱️ Time elapsed for {csv_file}: {elapsed:.2f} seconds\")\n",
    "\n",
    "        # log to file\n",
    "        log_time(csv_file, elapsed)\n",
    "\n",
    "        # Update the same tqdm bar\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Push final status to Google Sheet including Scraped Count\n",
    "    ws.update([df_sheet.columns.values.tolist()] +\n",
    "              df_sheet.fillna(\"\").values.tolist())\n",
    "    print(\"✅ Sheet updated with Scraping? and Scraped Count statuses\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run all CSVs\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_spider_on_csvs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
